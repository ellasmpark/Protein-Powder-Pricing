{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36b2fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import ast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60129fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',\n",
    "    'Accept-Language': 'de-DE,de;q=0.9'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96935f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Set up logging for notebook\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EnhancedProteinScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        })\n",
    "        self.base_url = 'https://muchproteins.com'\n",
    "        self.products_data = []\n",
    "        self.price_history_data = []\n",
    "        \n",
    "    def get_page(self, url, retries=3):\n",
    "        \"\"\"Get page content with error handling and retries\"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "                \n",
    "                response = self.session.get(url, timeout=15)\n",
    "                response.raise_for_status()\n",
    "                return response\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"‚ö†Ô∏è Attempt {attempt + 1} failed for {url}: {e}\")\n",
    "                if attempt == retries - 1:\n",
    "                    print(f\"‚ùå Failed to fetch {url} after {retries} attempts\")\n",
    "                    return None\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "    \n",
    "    def scrape_main_pages(self, start_page=1, max_pages=5):\n",
    "        \"\"\"Scrape the main listing pages to extract product data\"\"\"\n",
    "        print(f\"üöÄ Starting to scrape pages {start_page} to {max_pages}...\")\n",
    "        \n",
    "        for page in range(start_page, max_pages + 1):\n",
    "            if page == 1:\n",
    "                url = self.base_url\n",
    "            else:\n",
    "                url = f\"{self.base_url}/{page}\"\n",
    "            \n",
    "            print(f\"\\nüìÑ Scraping page {page}: {url}\")\n",
    "            \n",
    "            response = self.get_page(url)\n",
    "            if not response:\n",
    "                continue\n",
    "                \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            products_found = self.extract_products_from_listing(soup, page)\n",
    "            \n",
    "            if not products_found:\n",
    "                print(f\"‚ùå No products found on page {page}, stopping...\")\n",
    "                break\n",
    "            \n",
    "            print(f\"‚úÖ Found {products_found} products on page {page}\")\n",
    "                \n",
    "        print(f\"\\nüéâ Scraped {len(self.products_data)} products total\")\n",
    "        return len(self.products_data)\n",
    "    \n",
    "    def extract_products_from_listing(self, soup, page_num):\n",
    "        \"\"\"Extract product data from listing page\"\"\"\n",
    "        products_found = 0\n",
    "        \n",
    "        scripts = soup.find_all('script')\n",
    "        \n",
    "        for script in scripts:\n",
    "            if not script.string:\n",
    "                continue\n",
    "                \n",
    "            script_content = script.string.strip()\n",
    "            \n",
    "            if script_content.startswith('{\"props\":'):\n",
    "                try:\n",
    "                    data = json.loads(script_content)\n",
    "                    \n",
    "                    if 'props' in data and 'pageProps' in data['props']:\n",
    "                        page_props = data['props']['pageProps']\n",
    "                        \n",
    "                        if 'data' in page_props and isinstance(page_props['data'], list):\n",
    "                            products_data = page_props['data']\n",
    "                            print(f\"üéâ Found {len(products_data)} products in data array!\")\n",
    "                            \n",
    "                            for product_data in products_data:\n",
    "                                product = self.parse_muchproteins_product(product_data, page_num)\n",
    "                                if product:\n",
    "                                    self.products_data.append(product)\n",
    "                                    products_found += 1\n",
    "                                    \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"‚ùå Failed to parse React props JSON: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        return products_found\n",
    "    \n",
    "    def parse_muchproteins_product(self, product_data, page_num):\n",
    "        \"\"\"Parse product data from muchproteins.com specific format\"\"\"\n",
    "        if not isinstance(product_data, dict):\n",
    "            return None\n",
    "            \n",
    "        product = {\n",
    "            'source_page': page_num,\n",
    "            'scraped_at': datetime.now().isoformat(),\n",
    "            'extraction_method': 'muchproteins_json'\n",
    "        }\n",
    "        \n",
    "        # Map the actual fields from muchproteins.com (based on real data structure)\n",
    "        field_mappings = {\n",
    "            'name': 'title',\n",
    "            'url': 'product_url',\n",
    "            'manufacturer': 'brand',  # muchproteins uses 'manufacturer' not 'brand'\n",
    "            'protein': 'protein_per_100g',\n",
    "            'pricePerProtein100g': 'price_per_100g_protein',  # actual field name\n",
    "            'pricePer100g': 'price_per_100g',  # additional price field\n",
    "            'energy': 'energy_per_100g',\n",
    "            'type': 'product_type',\n",
    "            'tags': 'product_tags',  # muchproteins uses 'tags'\n",
    "            'categories': 'categories',\n",
    "            'flavour': 'flavor',\n",
    "            'flavors': 'flavors',\n",
    "            'weight': 'weight',\n",
    "            'pricePerKg': 'price_per_kg',\n",
    "            'priceTotal': 'total_price',\n",
    "            'rating': 'rating',\n",
    "            'reviews': 'review_count',\n",
    "            'slug': 'product_slug',\n",
    "            'imagePublicThumb': 'image_url',\n",
    "            'ingredientsMap': 'ingredients',\n",
    "            'nutritionMap': 'nutrition_info',\n",
    "            'priceSummary': 'price_summary'\n",
    "        }\n",
    "        \n",
    "        # Extract all available fields\n",
    "        for original_key, value in product_data.items():\n",
    "            mapped_key = field_mappings.get(original_key, original_key)\n",
    "            \n",
    "            # Handle different data types\n",
    "            if isinstance(value, list):\n",
    "                try:\n",
    "                    if value:\n",
    "                        flattened = []\n",
    "                        for item in value:\n",
    "                            if isinstance(item, list):\n",
    "                                flattened.extend([str(x) for x in item])\n",
    "                            else:\n",
    "                                flattened.append(str(item))\n",
    "                        product[mapped_key] = ', '.join(flattened)\n",
    "                    else:\n",
    "                        product[mapped_key] = ''\n",
    "                except Exception as e:\n",
    "                    product[mapped_key] = str(value)\n",
    "            elif isinstance(value, (str, int, float, bool)):\n",
    "                product[mapped_key] = value\n",
    "            elif value is not None:\n",
    "                product[mapped_key] = str(value)\n",
    "        \n",
    "        # Parse numeric values (using actual field names)\n",
    "        if 'protein_per_100g' in product:\n",
    "            product['protein_per_100g'] = self.parse_numeric(product['protein_per_100g'])\n",
    "        \n",
    "        if 'price_per_100g_protein' in product:\n",
    "            product['price_per_100g_protein'] = self.parse_numeric(product['price_per_100g_protein'])\n",
    "            \n",
    "        if 'price_per_100g' in product:\n",
    "            product['price_per_100g'] = self.parse_numeric(product['price_per_100g'])\n",
    "            \n",
    "        if 'energy_per_100g' in product:\n",
    "            product['energy_per_100g'] = self.parse_numeric(product['energy_per_100g'])\n",
    "        \n",
    "        # Create the muchproteins.com URL for this product\n",
    "        if 'slug' in product_data:\n",
    "            # Use the actual slug from the data\n",
    "            product['muchproteins_url'] = f\"/protein/{product_data['slug']}\"\n",
    "        elif 'name' in product_data and 'manufacturer' in product_data:\n",
    "            # Fallback: create URL from name and manufacturer\n",
    "            brand_slug = product_data['manufacturer'].lower().replace(' ', '-')\n",
    "            name_slug = product_data['name'].lower().replace(' ', '-').replace('‚Ñ¢', '').replace('¬Æ', '')\n",
    "            name_slug = re.sub(r'[^a-z0-9\\-]', '', name_slug)\n",
    "            product['muchproteins_url'] = f\"/protein/{brand_slug}/{name_slug}\"\n",
    "        \n",
    "        return product if product.get('title') else None\n",
    "    \n",
    "    def scrape_price_history(self, sample_size=10, delay_between_requests=3):\n",
    "        \"\"\"Scrape price history for a sample of products\"\"\"\n",
    "        if not self.products_data:\n",
    "            print(\"‚ùå No products available. Run scrape_main_pages() first.\")\n",
    "            return\n",
    "        \n",
    "        # Take a sample of products to avoid overwhelming the server\n",
    "        import random\n",
    "        sample_products = random.sample(self.products_data, min(sample_size, len(self.products_data)))\n",
    "        \n",
    "        print(f\"üìà Scraping price history for {len(sample_products)} sample products...\")\n",
    "        \n",
    "        successful_scrapes = 0\n",
    "        \n",
    "        for i, product in enumerate(sample_products):\n",
    "            if 'muchproteins_url' not in product:\n",
    "                print(f\"‚ö†Ô∏è Product {i+1}: No URL found, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            full_url = self.base_url + product['muchproteins_url']\n",
    "            print(f\"\\nüîç {i+1}/{len(sample_products)}: {product.get('title', 'Unknown')[:50]}...\")\n",
    "            print(f\"   URL: {full_url}\")\n",
    "            \n",
    "            price_history = self.scrape_individual_price_history(full_url, product)\n",
    "            \n",
    "            if price_history:\n",
    "                # Add price history to the product\n",
    "                product['price_history'] = price_history\n",
    "                \n",
    "                # Also store in separate list for analysis\n",
    "                for entry in price_history:\n",
    "                    history_record = {\n",
    "                        'product_title': product.get('title'),\n",
    "                        'product_brand': product.get('brand'),\n",
    "                        'muchproteins_url': product['muchproteins_url'],\n",
    "                        **entry\n",
    "                    }\n",
    "                    self.price_history_data.append(history_record)\n",
    "                \n",
    "                successful_scrapes += 1\n",
    "                print(f\"   ‚úÖ Found {len(price_history)} price data points\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå No price history found\")\n",
    "            \n",
    "            # Be respectful with delays\n",
    "            if i < len(sample_products) - 1:\n",
    "                time.sleep(delay_between_requests)\n",
    "        \n",
    "        print(f\"\\nüéâ Successfully scraped price history for {successful_scrapes}/{len(sample_products)} products\")\n",
    "        print(f\"üìä Total price data points collected: {len(self.price_history_data)}\")\n",
    "    \n",
    "    def scrape_individual_price_history(self, url, product_info):\n",
    "        \"\"\"Scrape price history from an individual product page\"\"\"\n",
    "        response = self.get_page(url)\n",
    "        if not response:\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Look for price history data in script tags\n",
    "        scripts = soup.find_all('script')\n",
    "        \n",
    "        for script in scripts:\n",
    "            if not script.string:\n",
    "                continue\n",
    "                \n",
    "            script_content = script.string.strip()\n",
    "            \n",
    "            # Method 1: Look for Next.js page data\n",
    "            if script_content.startswith('{\"props\":'):\n",
    "                try:\n",
    "                    data = json.loads(script_content)\n",
    "                    \n",
    "                    if 'props' in data and 'pageProps' in data['props']:\n",
    "                        page_props = data['props']['pageProps']\n",
    "                        \n",
    "                        # Look for price history data\n",
    "                        price_history = self.extract_price_history_from_props(page_props)\n",
    "                        if price_history:\n",
    "                            return price_history\n",
    "                            \n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            # Method 2: Look for chart data or time series data\n",
    "            if any(keyword in script_content.lower() for keyword in ['chart', 'price', 'history', 'data']):\n",
    "                price_history = self.extract_price_history_from_script(script_content)\n",
    "                if price_history:\n",
    "                    return price_history\n",
    "        \n",
    "        # Method 3: Look for structured data in HTML\n",
    "        return self.extract_price_history_from_html(soup)\n",
    "    \n",
    "    def extract_price_history_from_props(self, page_props):\n",
    "        \"\"\"Extract price history from Next.js page props\"\"\"\n",
    "        price_history = []\n",
    "        \n",
    "        # Look for common price history field names\n",
    "        history_fields = ['priceHistory', 'price_history', 'history', 'prices', 'chartData', 'data']\n",
    "        \n",
    "        for field in history_fields:\n",
    "            if field in page_props:\n",
    "                data = page_props[field]\n",
    "                if isinstance(data, list) and len(data) > 0:\n",
    "                    # Check if this looks like time series data\n",
    "                    first_item = data[0]\n",
    "                    if isinstance(first_item, dict) and any(key in first_item for key in ['date', 'time', 'price', 'value']):\n",
    "                        for item in data:\n",
    "                            price_entry = self.parse_price_history_entry(item)\n",
    "                            if price_entry:\n",
    "                                price_history.append(price_entry)\n",
    "                        return price_history\n",
    "        \n",
    "        # Also check nested objects\n",
    "        for key, value in page_props.items():\n",
    "            if isinstance(value, dict):\n",
    "                nested_history = self.extract_price_history_from_props(value)\n",
    "                if nested_history:\n",
    "                    return nested_history\n",
    "        \n",
    "        return price_history if price_history else None\n",
    "    \n",
    "    def extract_price_history_from_script(self, script_content):\n",
    "        \"\"\"Extract price history from script tags\"\"\"\n",
    "        price_history = []\n",
    "        \n",
    "        # Look for array patterns that might contain price data\n",
    "        patterns = [\n",
    "            r'priceHistory[\"\\']?\\s*:\\s*(\\[.+?\\])',\n",
    "            r'chartData[\"\\']?\\s*:\\s*(\\[.+?\\])',\n",
    "            r'prices[\"\\']?\\s*:\\s*(\\[.+?\\])',\n",
    "            r'data[\"\\']?\\s*:\\s*(\\[.+?\\])',\n",
    "            r'history[\"\\']?\\s*:\\s*(\\[.+?\\])'\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, script_content, re.DOTALL | re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                try:\n",
    "                    data = json.loads(match)\n",
    "                    if isinstance(data, list) and len(data) > 0:\n",
    "                        # Check if this looks like time series data\n",
    "                        first_item = data[0]\n",
    "                        if isinstance(first_item, dict) and any(key in str(first_item).lower() for key in ['date', 'price', 'time']):\n",
    "                            for item in data:\n",
    "                                price_entry = self.parse_price_history_entry(item)\n",
    "                                if price_entry:\n",
    "                                    price_history.append(price_entry)\n",
    "                            return price_history\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    continue\n",
    "        \n",
    "        return price_history if price_history else None\n",
    "    \n",
    "    def extract_price_history_from_html(self, soup):\n",
    "        \"\"\"Extract price history from HTML elements\"\"\"\n",
    "        # This would be used if the price data is embedded in HTML rather than JSON\n",
    "        # Look for canvas elements (charts), data attributes, or table data\n",
    "        \n",
    "        # Check for data attributes\n",
    "        elements_with_data = soup.find_all(attrs={'data-price': True}) or soup.find_all(attrs={'data-chart': True})\n",
    "        \n",
    "        if elements_with_data:\n",
    "            price_history = []\n",
    "            for element in elements_with_data:\n",
    "                # Try to extract data from attributes\n",
    "                for attr, value in element.attrs.items():\n",
    "                    if 'price' in attr.lower() or 'chart' in attr.lower():\n",
    "                        try:\n",
    "                            data = json.loads(value)\n",
    "                            if isinstance(data, list):\n",
    "                                for item in data:\n",
    "                                    price_entry = self.parse_price_history_entry(item)\n",
    "                                    if price_entry:\n",
    "                                        price_history.append(price_entry)\n",
    "                        except (json.JSONDecodeError, TypeError):\n",
    "                            continue\n",
    "            \n",
    "            return price_history if price_history else None\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def parse_price_history_entry(self, entry):\n",
    "        \"\"\"Parse a single price history entry\"\"\"\n",
    "        if not isinstance(entry, dict):\n",
    "            return None\n",
    "        \n",
    "        price_entry = {}\n",
    "        \n",
    "        # Map common field names\n",
    "        date_fields = ['date', 'time', 'timestamp', 'x']\n",
    "        price_fields = ['price', 'value', 'y', 'amount']\n",
    "        \n",
    "        # Extract date\n",
    "        for field in date_fields:\n",
    "            if field in entry:\n",
    "                price_entry['date'] = entry[field]\n",
    "                break\n",
    "        \n",
    "        # Extract price\n",
    "        for field in price_fields:\n",
    "            if field in entry:\n",
    "                price_entry['price'] = self.parse_numeric(entry[field])\n",
    "                break\n",
    "        \n",
    "        # Add any other fields that might be useful\n",
    "        for key, value in entry.items():\n",
    "            if key not in price_entry:\n",
    "                price_entry[key] = value\n",
    "        \n",
    "        # Validate that we have minimum required data\n",
    "        if 'date' in price_entry and 'price' in price_entry:\n",
    "            return price_entry\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def parse_numeric(self, value):\n",
    "        \"\"\"Extract numeric value from string\"\"\"\n",
    "        if isinstance(value, (int, float)):\n",
    "            return value\n",
    "        if isinstance(value, str):\n",
    "            cleaned = re.sub(r'[‚Ç¨$¬£,\\s]', '', value)\n",
    "            match = re.search(r'\\d+(?:\\.\\d+)?', cleaned)\n",
    "            if match:\n",
    "                return float(match.group())\n",
    "        return None\n",
    "    \n",
    "    def to_dataframe(self):\n",
    "        \"\"\"Convert scraped data to pandas DataFrame\"\"\"\n",
    "        if not self.products_data:\n",
    "            print(\"‚ö†Ô∏è No data to convert\")\n",
    "            return pd.DataFrame()\n",
    "        return pd.DataFrame(self.products_data)\n",
    "    \n",
    "    def price_history_to_dataframe(self):\n",
    "        \"\"\"Convert price history data to pandas DataFrame\"\"\"\n",
    "        if not self.price_history_data:\n",
    "            print(\"‚ö†Ô∏è No price history data to convert\")\n",
    "            return pd.DataFrame()\n",
    "        return pd.DataFrame(self.price_history_data)\n",
    "    \n",
    "    def save_data(self, products_filename='protein_products.json', history_filename='price_history.json'):\n",
    "        \"\"\"Save scraped data to JSON files\"\"\"\n",
    "        # Save products\n",
    "        with open(products_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.products_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"üíæ Products data saved to {products_filename}\")\n",
    "        \n",
    "        # Save price history\n",
    "        if self.price_history_data:\n",
    "            with open(history_filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.price_history_data, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"üíæ Price history data saved to {history_filename}\")\n",
    "\n",
    "# Usage example for notebook\n",
    "def demo_usage():\n",
    "    \"\"\"Example of how to use the enhanced scraper\"\"\"\n",
    "    \n",
    "    print(\"=== ENHANCED PROTEIN SCRAPER DEMO ===\\n\")\n",
    "    \n",
    "    # Step 1: Initialize scraper\n",
    "    scraper = EnhancedProteinScraper()\n",
    "    \n",
    "    # Step 2: Scrape main product listings\n",
    "    print(\"Step 1: Scraping main product listings...\")\n",
    "    scraper.scrape_main_pages(start_page=1, max_pages=2)\n",
    "    \n",
    "    # Step 3: Scrape price history for a sample\n",
    "    print(\"\\nStep 2: Scraping price history for sample products...\")\n",
    "    scraper.scrape_price_history(sample_size=5, delay_between_requests=2)\n",
    "    \n",
    "    # Step 4: Convert to DataFrames\n",
    "    products_df = scraper.to_dataframe()\n",
    "    history_df = scraper.price_history_to_dataframe()\n",
    "    \n",
    "    print(f\"\\nüìä Results:\")\n",
    "    print(f\"Products: {len(products_df)}\")\n",
    "    print(f\"Price history records: {len(history_df)}\")\n",
    "    \n",
    "    return scraper, products_df, history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afe0c5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Scraping main product listings...\n",
      "üöÄ Starting to scrape pages 1 to 5...\n",
      "\n",
      "üìÑ Scraping page 1: https://muchproteins.com\n",
      "üéâ Found 25 products in data array!\n",
      "‚úÖ Found 25 products on page 1\n",
      "\n",
      "üìÑ Scraping page 2: https://muchproteins.com/2\n",
      "üéâ Found 25 products in data array!\n",
      "‚úÖ Found 25 products on page 2\n",
      "\n",
      "üìÑ Scraping page 3: https://muchproteins.com/3\n",
      "üéâ Found 25 products in data array!\n",
      "‚úÖ Found 25 products on page 3\n",
      "\n",
      "üìÑ Scraping page 4: https://muchproteins.com/4\n",
      "üéâ Found 25 products in data array!\n",
      "‚úÖ Found 25 products on page 4\n",
      "\n",
      "üìÑ Scraping page 5: https://muchproteins.com/5\n",
      "üéâ Found 3 products in data array!\n",
      "‚úÖ Found 3 products on page 5\n",
      "\n",
      "üéâ Scraped 103 products total\n",
      "‚úÖ Found 103 products\n",
      "\n",
      "üìä Available columns: ['source_page', 'scraped_at', 'extraction_method', 'title', 'product_url', 'brand', 'image_url', 'product_slug', 'ingredients', 'nutrition_info', 'product_tags', 'price_per_100g', 'price_per_100g_protein', 'price_summary', 'muchproteins_url']\n",
      "\n",
      "üéØ Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>price_per_100g_protein</th>\n",
       "      <th>price_per_100g</th>\n",
       "      <th>product_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Diet Whey Protein</td>\n",
       "      <td>PhD Nutrition</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.60</td>\n",
       "      <td>vegetarian, gluten free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brown Rice Protein</td>\n",
       "      <td>Myprotein</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.28</td>\n",
       "      <td>vegetarian, vegan, gluten free, dairy free, la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Impact Soy Protein</td>\n",
       "      <td>Myprotein</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.95</td>\n",
       "      <td>vegetarian, vegan, dairy free, lactose free, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Protein Hot Chocolate</td>\n",
       "      <td>Myprotein</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1.25</td>\n",
       "      <td>gluten free, vegetarian, dairy free, lactose f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Smart Protein Plant</td>\n",
       "      <td>PhD Nutrition</td>\n",
       "      <td>2.28</td>\n",
       "      <td>1.80</td>\n",
       "      <td>vegetarian, dairy free, lactose free, vegan, g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title          brand  price_per_100g_protein  \\\n",
       "0     Diet Whey Protein   PhD Nutrition                    0.88   \n",
       "1    Brown Rice Protein       Myprotein                    1.64   \n",
       "2     Impact Soy Protein      Myprotein                    2.17   \n",
       "3  Protein Hot Chocolate      Myprotein                    2.19   \n",
       "4    Smart Protein Plant  PhD Nutrition                    2.28   \n",
       "\n",
       "   price_per_100g                                       product_tags  \n",
       "0            0.60                            vegetarian, gluten free  \n",
       "1            1.28  vegetarian, vegan, gluten free, dairy free, la...  \n",
       "2            1.95  vegetarian, vegan, dairy free, lactose free, g...  \n",
       "3            1.25  gluten free, vegetarian, dairy free, lactose f...  \n",
       "4            1.80  vegetarian, dairy free, lactose free, vegan, g...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü•ó Sample nutrition data:\n",
      "{'typical values (when mixed with water¬±)': 'per 100g', 'energy': {'kj': 1528, 'kcal': 365}, 'fat': {'all': 5.5, 'saturated': 1.6}, 'carbohydrates': {'all': 11, 'sugar': 5}, 'fibre': 3, 'protein': 68, 'salt': 0.63, 'also provides:': None, 'flaxseed powder': {'all': 3, 'of which is flaxseed oil': 1.2}, 'conjugated linoleic acid powder': {'all': 1.5, 'of which is conjugated linoleic acid': 1}, 'l-carnitine': 0.5, 'green tea extract': 0.4}\n",
      "\n",
      "üìã Sample ingredients data:\n",
      "PhD Premium Protein Blend, ['Whey* Protein Concentrate', ['Milk Protein Concentrate', ['Of Which 80% Is Micellar Casein']], 'Soya Protein Isolate'], Waxy Barley Flour, Golden Brown Flaxseed Powder, Thickeners, ['Acacia Gum', 'Guar Gum', 'Xanthan Gum'], Flavouring, Conjugated Linoleic Acid Powder, [['Safflower Oil', ['Rich in Conjugated Linoeic Acid']], 'Glucose Syrup', 'Milk Protein', ['Emulsifier', ['Soya Lecithin']], 'Vitamin E'], L-Carnitine, Green Tea Extract, Sweetener, ['Sucralose']\n",
      "\n",
      "üîó Sample URLs for price history:\n",
      "  1. https://muchproteins.com/protein/diet-whey-protein\n",
      "  2. https://muchproteins.com/protein/brown-rice-protein\n",
      "  3. https://muchproteins.com/protein/impact-soy-protein\n"
     ]
    }
   ],
   "source": [
    "# Initialize the enhanced scraper\n",
    "scraper = EnhancedProteinScraper()\n",
    "\n",
    "# First, get the main product listings\n",
    "print(\"üöÄ Scraping main product listings...\")\n",
    "total_products = scraper.scrape_main_pages(start_page=1, max_pages=5)\n",
    "\n",
    "# Check the products with the correct column names\n",
    "products_df = scraper.to_dataframe()\n",
    "print(f\"‚úÖ Found {len(products_df)} products\")\n",
    "\n",
    "if not products_df.empty:\n",
    "    print(f\"\\nüìä Available columns: {list(products_df.columns)}\")\n",
    "    \n",
    "    # Display with the actual column names that exist\n",
    "    display_columns = ['title', 'brand', 'price_per_100g_protein', 'price_per_100g', 'product_tags']\n",
    "    \n",
    "    # Check which columns actually exist\n",
    "    existing_columns = [col for col in display_columns if col in products_df.columns]\n",
    "    \n",
    "    print(f\"\\nüéØ Sample data:\")\n",
    "    display(products_df[existing_columns].head())\n",
    "    \n",
    "    # Show nutrition and ingredient data if available\n",
    "    if 'nutrition_info' in products_df.columns:\n",
    "        print(f\"\\nü•ó Sample nutrition data:\")\n",
    "        print(products_df['nutrition_info'].iloc[0])\n",
    "    \n",
    "    if 'ingredients' in products_df.columns:\n",
    "        print(f\"\\nüìã Sample ingredients data:\")\n",
    "        print(products_df['ingredients'].iloc[0])\n",
    "        \n",
    "    # Show the URLs we can use for price history\n",
    "    if 'muchproteins_url' in products_df.columns:\n",
    "        print(f\"\\nüîó Sample URLs for price history:\")\n",
    "        for i, url in enumerate(products_df['muchproteins_url'].head(3)):\n",
    "            print(f\"  {i+1}. https://muchproteins.com{url}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No products found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b962ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "products_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea000a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Save as CSV\n",
    "# products_df.to_csv('product_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
